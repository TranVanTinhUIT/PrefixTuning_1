python run_language_modeling.py         
--output_dir=webnlg_models/webnlgprefixtune_y_3_act_cat_b=5-e=1_d=0.0_u=no_lr=5e-05_w=0.0_s=101_r=n_m=512_o=1_o=1         
--model_type=gpt2         
--model_name_or_path=gpt2-medium         
--tokenizer_name=gpt2-medium         
--per_device_train_batch_size 5         
--per_device_eval_batch_size 5         
--save_steps 500000         
--num_train_epochs 1         
--do_train         
--train_data_file=/content/PrefixTuning_1/data/webnlg_challenge_2017/train.json         
--do_eval         
--line_by_line         
--save_total_limit 1         
--overwrite_output_dir         
--task_mode webnlg         
--eval_data_file=/content/PrefixTuning_1/data/webnlg_challenge_2017/dev.json          
--tuning_mode prefixtune 
--logging_dir webnlg_models/runs/webnlgprefixtune_y_3_act_cat_b=5-e=1_d=0.0_u=no_lr=5e-05_w=0.0_s=101_r=n_m=512_o=1_o=1         
--train_embs no 
--optim_prefix yes 
--preseqlen 3 
--prefix_mode activation 
--format_mode cat 
--gradient_accumulation_steps 1 
--learning_rate 5e-05 
--weight_decay 0.0 
--seed 101 
--disable_tqdm 
--mid_dim 512 
--init_random no --use_dropout no --prefix_dropout 0.0 --objective_mode 1 
--evaluate_during_training 
--eval_steps 5000  
--cache_dir /content/PrefixTuning_1/output/contrast_LM/transformers/examples/control/gpt2-medium-s3 
